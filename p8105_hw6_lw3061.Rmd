---
title: "Homework 6"
author: "Lucia Wang (lw3061)"
due date: "December 12, 2023"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(modelr)
library(purrr)

knitr::opts_chunk$set(echo = TRUE)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

## Problem 2
First, download data:
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

The simple linear regression will have `tmax` as the response variable with `tmin` and `prcp` as predictors. Run 5000 bootstraps and estimate r-squared and log(beta1 * beta2), then plot the distributions.

```{r}
weather_bstrap = weather_df |>
  bootstrap(n=5000) |>
  mutate(models = map(strap, \(df) lm(tmax ~ tmin + prcp, data=df) ),
         results = map(models, broom::tidy),
         rsq = map(models, broom::glance)
         ) |>
  select(-strap, -models) |>
  unnest(results) |> 
  filter(term %in% c("tmin", "prcp")) |>
  group_by(.id) |> 
  mutate(beta1x2 = prod(estimate)) |>
  select(beta1x2, rsq) |> 
  unnest(rsq) |> 
  janitor::clean_names() |>
  select(id, beta1x2, r_squared) |> unique()
```

```{r}
weather_bstrap |> ggplot(aes(x=beta1x2)) + geom_density()
```

The distribution of the log(beta1 * beta2) estimates seems to be centered around 0 with majority of values between -0.01 and 0.01.

```{r}
weather_bstrap |> ggplot(aes(x=r_squared)) + geom_density()
```

The distribution of the r-squared values is pretty high at around 0.92. The bulk of the estimates fall approximately from 0.9 to 0.95 but skew left (more higher values). 

Now calculate the confidence intervals for the sampled log(beta1 * beta2) and r-squared values...
```{r}
sum_weather = weather_bstrap |>
  unique() |> ungroup() |> select(-id) |> summarize(betas_mean = mean(beta1x2),
                                              betas_low = quantile(beta1x2, 0.025),
                                              betas_high = quantile(beta1x2, 0.975),
                                              rsq_mean = mean(r_squared),
                                              rsq_low = quantile(r_squared, 0.025),
                                              rsq_high = quantile(r_squared, 0.975))
 sum_weather |> knitr::kable()
```

After 5000 bootstraps, the 95% CI for the r-squared values is (`r sum_weather$rsq_low`, `r sum_weather$rsq_high`) and the CI for the log(beta1 * beta2) values is (`r sum_weather$betas_low`, `r sum_weather$betas_high`).

## Problem 3
Import and tidy birthweight data:
```{r}
bw = read_csv("birthweight.csv") |>
  mutate(babysex= as.factor(babysex),
         frace = as.factor(frace),
         malform = as.factor(malform),
         mrace = as.factor(mrace))
```

My proposed regression model for birthweight will use mother's weight at delivery `delwt`, mother's age at delivery `momage`, and mother's age at menarche `menarche` as predictors, with no interaction terms. I am curious about these variables because they are related to reproductive cycles of the mother so they would likely affect the development of the fetus and thus the birthweight of the baby.

```{r}
fit = lm(bwt ~ delwt + momage + menarche, data=bw)
fit |> broom::tidy() |> select(term, estimate, p.value) |> knitr::kable(digits=3)
```

At a quick glance the estimates have low p-values except for the estimate for `menarche`. Now I will add residuals and fitted values...

```{r}
bw |>
  modelr::add_residuals(fit) |>
  modelr::add_predictions(fit) |>
  ggplot(aes(x=resid, y=pred)) + geom_point(alpha=0.5, size=0.5)
```

The plot of the fitted values and residuals is not looking very good; there are some outliers and some clustering of values, so maybe the linear model isn't the best. 

Now I'll compare this to 2 other models using the cross-validation method:
```{r}
cv_df = crossv_mc(bw, 10)

cv_df = cv_df |>
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble)) 

cv_df = cv_df |>
  mutate(
    my_mod = map(train, \(df) lm(bwt ~ delwt + momage + menarche, data=df)),
    main_mod = map(train, \(df) lm(bwt ~ blength + gaweeks, data=df)),
    interact_mod = map(train, \(df) lm(bwt ~ bhead * blength * babysex, data=df))
    ) |>
  mutate(
    rmse_my = map2_dbl(my_mod, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_main = map2_dbl(main_mod, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_interact = map2_dbl(interact_mod, test, \(mod, df) rmse(model = mod, data = df))
    )

cv_df |> select(starts_with("rmse")) |>
  pivot_longer(
    everything(), names_to="model", values_to="rmse", names_prefix="rmse_"
  ) |>
  ggplot(aes(x=model, y=rmse)) + geom_violin()
```

Based on looking at the root mean squared error, the model with the interactions between `bhead`, `blength`, and `babysex` seems to be the best fit as it has the smallest rmse. In contrast, the rmse for my model is really much higher than the other two models, so it looks like it's not optimal. 

