---
title: "Homework 6"
author: "Lucia Wang (lw3061)"
due date: "December 12, 2023"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(modelr)
library(purrr)

knitr::opts_chunk$set(echo = TRUE)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1
Import and tidy homicides data as before:
```{r}
homicide = 
  read_csv("homicide-data.csv") |> 
  janitor::clean_names() |>
  separate(reported_date, into=c("year","month","day"), sep=c(4,6)) |>
  mutate(city_state = str_c(city, ", ", state)) |>
  filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"))  |> 
  mutate(status = case_when(
    disposition %in% c("Closed without arrest","Open/No arrest") ~ "unsolved",
    disposition == "Closed by arrest" ~ "solved"
  )) |>
  filter(victim_race %in% c("White", "Black")) |>
  mutate(victim_age = as.numeric(victim_age),
         resolved = as.numeric(disposition == "Closed by arrest"),
         victim_race = fct_relevel(victim_race, "White"))
```

Now to run a logistic regression on homicides vs age, sex, and race in Baltimore, MD.

```{r}
md_glm = homicide |> filter(city_state =="Baltimore, MD") |>
  glm(resolved ~ victim_age + victim_sex + victim_race, data=_, family=binomial())

md_or = md_glm |> broom::tidy() |>
  mutate(
    OR = exp(estimate),
    OR_low = exp(estimate - 1.96*std.error),
    OR_high = exp(estimate + 1.96*std.error)) |>
  select(term,OR, OR_low, OR_high)

md_or_sex = md_or |> filter(term=="victim_sexMale") 

md_or_sex|> knitr::kable()
```

The adjusted OR for solving homicides comparing male victims to female victims with others variables fixed is `r round(md_or_sex$OR, digits=2)` with 95% confidence interval of (`r round(md_or_sex$OR_low, digits=2)`, `r round(md_or_sex$OR_high, digits=2)`). Male victims are `r 1 - round(md_or_sex$OR, digits=2)` less likely to have their homicide resolved than female victims adjusted for age and race.

Here I run the same regression using `glm` on all the cities in the dataset:
```{r}
all_cities = homicide |> nest(.by=city_state) |>
  mutate(
  models = map(data, 
               \(df) glm(resolved ~ victim_age + victim_sex + victim_race,
                         family=binomial(), data=df)),
  models = map(models, broom::tidy)
) |>
  select(-data) |>
  unnest(cols=models) |>
  mutate(
    OR = exp(estimate),
    OR_low = exp(estimate - 1.96*std.error),
    OR_high = exp(estimate + 1.96*std.error)) |>
  filter(term=="victim_sexMale") |>
  select(city_state, OR, OR_low, OR_high)
```

This is the plot of ORs and 95% CIs organized from least to greatest OR.
```{r}
all_cities |> mutate(city_state=fct_reorder(city_state, OR)) |>
  ggplot(aes(x=city_state, y=OR)) +
  geom_point() +
  geom_errorbar(aes(ymin=OR_low, ymax=OR_high)) +
   theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

The smallest OR is in NYC with a value of `r round(filter(all_cities, city_state=="New York, NY")$OR, digits=2)`, suggesting that male victims are `r 1-round(filter(all_cities, city_state=="New York, NY")$OR, digits=2)` less likely than females to have their homicide resolved after adjusting for age and race. The highest OR is Albuquerque with a value of `r round(filter(all_cities, city_state=="Albuquerque, NM")$OR, digits=2)`. Since this is above 1, males are `r 1-round(filter(all_cities, city_state=="Albuquerque, NM")$OR, digits=2)` times more likely to have their homicides resolved. The CIs for the higher ORs (around 1- above 1) are much wider than the ones with ORs below 1.

## Problem 2
First, download data:
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

The simple linear regression will have `tmax` as the response variable with `tmin` and `prcp` as predictors. Run 5000 bootstraps and estimate r-squared and log(beta1 x beta2) - which is log(beta1) x log(beta2), then plot the distributions.

```{r}
weather_bstrap = weather_df |>
  bootstrap(n=5000) |>
  mutate(models = map(strap, \(df) lm(tmax ~ tmin + prcp, data=df) ),
         results = map(models, broom::tidy),
         rsq = map(models, broom::glance)
         ) |>
  select(-strap, -models) |>
  unnest(results) |> 
  filter(term %in% c("tmin", "prcp")) |>
  group_by(.id) |> 
  mutate(beta1x2 = sum(estimate)) |>
  select(beta1x2, rsq) |> 
  unnest(rsq) |> 
  janitor::clean_names() |>
  select(id, beta1x2, r_squared) |> unique()
```

```{r}
weather_bstrap |> ggplot(aes(x=beta1x2)) + geom_density()
```

The distribution of the log(beta1 * beta2) estimates seems to be centered around 1.01 with majority of values between 0.95 and 1.05.

```{r}
weather_bstrap |> ggplot(aes(x=r_squared)) + geom_density()
```

The distribution of the r-squared values is pretty high at around 0.92. The bulk of the estimates fall approximately from 0.9 to 0.95 but skew left (more higher values). 

Now calculate the confidence intervals for the sampled log(beta1 * beta2) and r-squared values...
```{r}
sum_weather = weather_bstrap |>
  unique() |> ungroup() |> select(-id) |> summarize(betas_mean = mean(beta1x2),
                                              betas_low = quantile(beta1x2, 0.025),
                                              betas_high = quantile(beta1x2, 0.975),
                                              rsq_mean = mean(r_squared),
                                              rsq_low = quantile(r_squared, 0.025),
                                              rsq_high = quantile(r_squared, 0.975))
 sum_weather |> knitr::kable()
```

After 5000 bootstraps, the 95% CI for the r-squared values is (`r sum_weather$rsq_low`, `r sum_weather$rsq_high`) and the CI for the log(beta1 * beta2) values is (`r sum_weather$betas_low`, `r sum_weather$betas_high`).

## Problem 3
Import and tidy birthweight data:
```{r}
bw = read_csv("birthweight.csv") |>
  mutate(babysex= as.factor(babysex),
         frace = as.factor(frace),
         malform = as.factor(malform),
         mrace = as.factor(mrace))
```

My proposed regression model for birthweight will use mother's weight at delivery `delwt`, mother's age at delivery `momage`, and mother's age at menarche `menarche` as predictors, with no interaction terms. I am curious about these variables because they are related to reproductive cycles of the mother so they would likely affect the development of the fetus and thus the birthweight of the baby.

```{r}
fit = lm(bwt ~ delwt + momage + menarche, data=bw)
fit |> broom::tidy() |> select(term, estimate, p.value) |> knitr::kable(digits=3)
```

At a quick glance the estimates have low p-values except for the estimate for `menarche`. Now I will add residuals and fitted values...

```{r}
bw |>
  modelr::add_residuals(fit) |>
  modelr::add_predictions(fit) |>
  ggplot(aes(x=resid, y=pred)) + geom_point(alpha=0.5, size=0.5)
```

The plot of the fitted values and residuals is not looking very good; there are some outliers and some clustering of values, so maybe the linear model isn't the best. 

Now I'll compare this to 2 other models using the cross-validation method:
```{r}
cv_df = crossv_mc(bw, 10)

cv_df = cv_df |>
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble)) 

cv_df = cv_df |>
  mutate(
    my_mod = map(train, \(df) lm(bwt ~ delwt + momage + menarche, data=df)),
    main_mod = map(train, \(df) lm(bwt ~ blength + gaweeks, data=df)),
    interact_mod = map(train, \(df) lm(bwt ~ bhead * blength * babysex, data=df))
    ) |>
  mutate(
    rmse_my = map2_dbl(my_mod, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_main = map2_dbl(main_mod, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_interact = map2_dbl(interact_mod, test, \(mod, df) rmse(model = mod, data = df))
    )

cv_df |> select(starts_with("rmse")) |>
  pivot_longer(
    everything(), names_to="model", values_to="rmse", names_prefix="rmse_"
  ) |>
  ggplot(aes(x=model, y=rmse)) + geom_violin()
```

Based on looking at the root mean squared error, the model with the interactions between `bhead`, `blength`, and `babysex` seems to be the best fit as it has the smallest rmse. In contrast, the rmse for my model is really much higher than the other two models, so it looks like it's not optimal. 

